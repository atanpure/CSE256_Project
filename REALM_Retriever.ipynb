{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "pZfpk-bhcgby"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_AZBZbvDm06"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "!pip install rouge\n",
        "\n",
        "\n",
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "import evaluate  # Bleu\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration, T5TokenizerFast\n",
        "\n",
        "from transformers import RealmConfig, RealmEmbedder, AutoTokenizer, RealmKnowledgeAugEncoder, RealmScorer\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REALM"
      ],
      "metadata": {
        "id": "txevrE6XpNxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"data_withNeg1k-5Cols.csv\")\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "MH0ePhb48qMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input_texts = [\"How are you?\", \"Hi\", \"Hello\"]\n",
        "#candidates_texts = [\"A cute cat.\", \"Nice to meet you!\"]\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "candidates_texts = []\n",
        "input_texts = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    sentences = []\n",
        "    input_texts.append(row['question'])\n",
        "    sentences.append(row['context'])\n",
        "    sentences.append(row['neg_context0'])\n",
        "    sentences.append(row['neg_context1'])\n",
        "    sentences.append(row['neg_context2'])\n",
        "    sentences.append(row['neg_context3'])\n",
        "    sentences.append(row['neg_context4'])\n",
        "    candidates_texts.append(sentences)\n",
        "\n",
        "print(len(input_texts))\n",
        "print(len(candidates_texts[0]))"
      ],
      "metadata": {
        "id": "KOu_D-nn8yjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, RealmScorer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/realm-cc-news-pretrained-scorer\")\n",
        "model = RealmScorer.from_pretrained(\"google/realm-cc-news-pretrained-scorer\", num_candidates=6)\n",
        "\n",
        "relevance_scores = []\n",
        "\n",
        "for idx in range(input_texts):\n",
        "  input_trunc = ' '.join(input_texts[idx].split()[:512])\n",
        "  candidates_trunc = [' '.join(cand.split()[:512]) for cand in candidates_texts[idx][:]]\n",
        "  inputs = tokenizer(input_trunc, max_length=512, padding=\"max_length\", truncation=True, pad_to_max_length=True, return_tensors=\"pt\")\n",
        "  candidates_inputs = tokenizer.batch_encode_candidates(candidates_trunc, max_length=512, padding=\"max_length\", truncation=True, pad_to_max_length=True, return_tensors=\"pt\")\n",
        "\n",
        "  outputs = model(\n",
        "      **inputs,\n",
        "      candidate_input_ids=candidates_inputs.input_ids,\n",
        "      candidate_attention_mask=candidates_inputs.attention_mask,\n",
        "      candidate_token_type_ids=candidates_inputs.token_type_ids,\n",
        "  )\n",
        "  relevance_scores.append(outputs.relevance_score)"
      ],
      "metadata": {
        "id": "N6JhcUlQ54k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Analysis"
      ],
      "metadata": {
        "id": "IMSP_ft8_MzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv(\"data_withNeg2k_realm.csv\")\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "-H5Dvp0e_L8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[0]"
      ],
      "metadata": {
        "id": "fHiZpeXvDcJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_preds = df[(df['relevantContext'] > 0)]\n",
        "len(wrong_preds)\n",
        "\n",
        "\n",
        "lengths = [900, 1000, 1100, 1000, 1200, 1300, 1350, 1400, 1450]\n",
        "wrongs = [len(wrong_preds[wrong_preds['n_tokens'] > l])/len(df[df['n_tokens'] > l]) for l in lengths]\n",
        "\n",
        "# sns.histplot(y=wrongs, x=lengths)\n",
        "# plt.title('Wrong predictions vs context length')\n",
        "\n",
        "fig = plt.subplots(figsize = (8, 5))\n",
        "\n",
        "# creating the bar plot\n",
        "plt.bar(lengths, wrongs, width=40)\n",
        "\n",
        "plt.xlabel(\"Length of context\")\n",
        "plt.ylabel(\"Proportion of wrong predictions\")\n",
        "plt.title(\"Wrong predictions vs context length\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HfnvZISZAT3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.subplots(figsize = (8, 5))\n",
        "\n",
        "lengths = [100, 200, 300, 400, 500, 600, 700, 800]\n",
        "wrongs = [len(wrong_preds[wrong_preds['token_length_answer'] > l])/len(df[df['token_length_answer'] > l]) for l in lengths]\n",
        "\n",
        "plt.title('Wrong predictions vs answer length')\n",
        "\n",
        "# sns.histplot(y=wrongs, x=lengths)\n",
        "\n",
        "# creating the bar plot\n",
        "plt.bar(lengths, wrongs, width=40)\n",
        "\n",
        "plt.xlabel(\"Length of answer\")\n",
        "plt.ylabel(\"Proportion of wrong predictions\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qVGSzcZVU5NO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}